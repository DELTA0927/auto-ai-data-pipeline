'파이프라인(Pipeline)'의 정의
이 프로젝트에서 '파이프라인' 은 특정 목적(AI 학습 데이터 준비)을 위해 데이터가 여러 단계를 거쳐 흐르고, 각 단계마다 데이터가 변환되거나 처리되는 일련의 자동화된 과정을 의미합니다. 마치 실제 파이프가 물을 한 곳에서 다른 곳으로 운반하듯, 데이터가 여러 처리 단계를 거쳐 최종 목적지(AI 학습 모델)에 도달하는 흐름을 비유하는 표현입니다.


프로젝트 정의 : 'AI 학습용 자동 데이터 파이프라인 (Automated AI Data Pipeline)'
AI 학습용 자동 데이터 파이프라인 (Automated AI Data Pipeline, 이하 자동화 파이프라인)은 웹에서 AI 학습에 필요한 데이터를 자동으로 가져오고(수집), 학습에 적합한 형태로 변환하고(정제/가공), 최종적으로 AI모델에 바로 사용할 수 있도록 정제하는 '자동화된 데이터 흐름(파이프라인)'을 만드는 것입니다.

주요 특징:
순차 처리 : 데이터는 한 단계가 완료되면 다음 단계로 넘어가며 순차적으로 처리됩니다.
자동화 : 각 단계의 처리 과정이 사람의 개입 없이 자동으로 이루어집니다.
데이터 변환/처리 : 각 단계는 데이터를 정제하거나, 가공하거나, 새로운 정보를 추가하는 등 데이터를 필요한 형태로 변환합니다.
효율성 : 복잡하고 반복적인 작업을 자동화하여 시간과 노력을 절약하고 효율성을 높입니다.

프로젝트인 **'자동화 파이프라인'**에서는 다음과 같은 단계들이 파이프라인을 구성합니다.
ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ
데이터 수집 -> 데이터 정제 -> 데이터 라벨링 -> 데이터 2차 정제 - > 결과물 자동 저장
ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ
데이터 수집 (웹 크롤링): 웹에서 원본 텍스트 데이터를 가져오는 단계입니다.
초기 데이터 정제: 수집된 텍스트에서 불필요한 요소를 제거하여 깨끗한 텍스트로 만드는 단계입니다.
콘텐츠 기반 카테고리 추론 및 자동 라벨링: 정제된 텍스트를 AI 모델이 **'추론'**하여 적합한 카테고리(라벨)를 부여하는 핵심 단계입니다.
초기 데이터 2차 정제 및 저장 : 라벨링된 데이터를 AI 학습에 적합한 표준 형식으로 최종 변환하여 저장하는 단계입니다.
이러한 단계들이 서로 연결되어 하나의 자동화된 데이터 흐름을 형성하며, 이 전체 과정을 **'파이프라인'**이라고 부르는 것
여기서 가장 중요한 건, 이 파이프라인 자체가 'AI의 추론' 개념을 포함하고 있다는 점입니다.

핵심 목표:
웹에서 텍스트 데이터를 수집한 후, 그 데이터를 AI 모델이 스스로 내용을 '추론'하여 카테고리(라벨)를 부여하고,
AI 학습에 바로 사용될 수 있는 정돈된 데이터셋으로 최종 가공하는 파이썬 스크립트 || 모듈입니다.

작동 방식 : 이 '자동화 파이프라인'은 다음과 같은 주요 단계로 이루어짐.

데이터 수집 (Web Scraping):특정 웹사이트(뉴스 기사, 블로그 등)에서 AI 학습에 필요한 텍스트 콘텐츠를 자동으로 가져옵니다.
초기 데이터 정제 (Preprocessing):수집된 텍스트에서 HTML 태그, 광고 문구, 불필요한 공백 등을 제거하여 깨끗한 텍스트 데이터로 만듭니다.
소규모 학습 : 파이프라인 내부에, 아주 적은 수의 수동으로 분류된 예시 텍스트 데이터(예: 각 주제별 5~10개 문장)를 가지고 간단한 머신러닝 분류 모델(예: scikit-learn의 Naive Bayes나 Logistic Regression)을 즉석에서 학습시킵니다. 이 학습은 몇 초 이내로 빠르게 완료됩니다.
자동 추론 및 라벨링: 학습된 이 모델은 새로 수집된 웹 문서의 내용을 분석하여 **가장 적합한 주제 카테고리를 '추론'**하고, 그 추론 결과에 따라 해당 문서에 자동으로 '라벨(정답 카테고리)'을 부여합니다.
2차 정제 : 라벨링 된 텍스트 데이터를 라벨에 맞게 간단 요약 및 내용을 기반 간략히 재구성하여 텍스트의 길이를 압축.
학습용 데이터셋 가공 및 저장 (Formatting & Storage):추론된 라벨과 원본 텍스트를 함께 묶어, AI 모델이 쉽게 불러와 학습할 수 있는 표준화된 형식(예: CSV 또는 JSON 파일)으로 변환하고 저장합니다.


<aside>

## AI 학습용 자동 데이터 파이프라인 (Automated AI Data Pipeline) MVP 개발 개요
본 문서는 **'AI 학습용 자동 데이터 파이프라인 (Automated AI Data Pipeline)' 프로젝트의 MVP(Minimum Viable Product, 최소 기능 제품) 개발 방향성**을 명확히 정의하고, 핵심적인 기술 선택 및 구현 전략을 공유하기 위해 작성되었습니다. 이 문서는 향후 프로젝트에 참여할 모든 이해관계자(개발자, 기획자, AI 모델러 등)가 프로젝트의 목표와 기술 스택을 쉽게 이해하고 기여할 수 있도록 돕는 것을 목표로 합니다.
---
### 1. 프로젝트 개요
**프로젝트명:** AI 학습용 자동 데이터 파이프라인 (Automated AI Data Pipeline)
**최종 목표:** 웹에서 AI 모델 학습에 필요한 데이터를 **자동으로 수집, 정제, 라벨링**하여 AI 모델에 바로 활용할 수 있는 **정돈된 데이터셋을 생성**하는 파이썬 기반의 '자동화된 데이터 흐름(파이프라인)' 모듈을 구축합니다.
**핵심 특징:**
- **순차 처리:** 데이터는 수집부터 저장까지 정의된 여러 단계를 순차적으로 거칩니다.
- **자동화:** 각 처리 단계는 사람의 개입 없이 자동으로 이루어집니다.
- **데이터 변환/처리:** 각 단계에서 데이터는 정제, 가공, 라벨링 등의 과정을 통해 필요한 형태로 변환됩니다.
- **효율성:** 복잡하고 반복적인 데이터 준비 작업을 자동화하여 AI 개발의 시간과 노력을 절약합니다.
---
### 2. '파이프라인' 및 'AI 추론 모듈' 정의

**2.1. '파이프라인(Pipeline)'의 정의:**
본 프로젝트에서 '파이프라인'은 **AI 학습 데이터 준비**라는 특정 목적을 위해 데이터가 여러 단계를 거쳐 흐르고, 각 단계마다 데이터가 변환되거나 처리되는 일련의 자동화된 과정을 의미합니다.

**우리 프로젝트의 파이프라인 단계:**

1. **데이터 수집 (웹 크롤링):** 웹에서 원본 텍스트 데이터를 가져옵니다.
2. **초기 데이터 정제:** 수집된 텍스트에서 불필요한 요소(HTML 태그, 광고 문구, 과도한 공백, 특수 문자 등)를 제거하여 깨끗한 텍스트로 만듭니다.
3. **콘텐츠 기반 카테고리 추론 및 자동 라벨링:** 정제된 텍스트를 **AI 모듈이 스스로 내용을 '추론'하여 적합한 카테고리(라벨)를 부여하는 핵심 단계**입니다.
4. **학습용 데이터셋 가공 및 저장:** 라벨링된 데이터를 AI 학습에 적합한 표준 형식으로 최종 변환하여 파일로 저장합니다.

**2.2. 'AI 추론 모듈'의 정의 및 역할:**
이 파이프라인의 핵심인 **'콘텐츠 기반 카테고리 추론 및 자동 라벨링' 단계에 포함될 '작은 AI 모듈'**을 의미합니다. 이 모듈의 주된 역할은 다음과 같습니다.

- **소규모 학습:** 파이프라인 내에서, 아주 적은 수의 수동으로 분류된 예시 텍스트 데이터(예: 각 주제별 5~10개 문장)를 가지고 **간단한 머신러닝 분류 모델을 즉석에서 학습**시킵니다.
- **자동 추론 및 라벨링:** 학습된 모델은 새로 수집된 웹 문서의 내용을 분석하여 **가장 적합한 주제 카테고리를 '추론'**하고, 해당 문서에 자동으로 라벨을 부여합니다.
- **목표:** 이 모듈은 복잡한 대화나 창의적인 추론을 직접 수행하는 것이 아니라, **데이터의 '내용'을 기반으로 '정확하게 분류하고 라벨링하는 도구'**로서의 역할을 수행합니다. 성공 기준은 '데이터 라벨링의 정확성'과 '파이프라인 처리 속도'입니다.

---

### 3. 기술 스택 및 구현 전략 (MVP 기준)

파이프라인 내의 'AI 추론 모듈'을 포함한 전체 시스템은 **전통적인 머신러닝 방식**을 기반으로 구축하며, 이는 MVP 단계에서 가장 효율적이고 현실적인 선택으로 판단됩니다.

**3.1. 핵심 기술 스택:**

- **언어:** Python
- **웹 데이터 수집:** `requests`, `BeautifulSoup` (웹 크롤링)
- **텍스트 정제:** `re` (정규 표현식)
- **텍스트 벡터화:** `scikit-learn`의 `TfidfVectorizer` (텍스트를 숫자 벡터로 변환)
- **AI 추론 (분류 모델):** `scikit-learn`의 `LogisticRegression` 또는 `MultinomialNB` (소규모 데이터셋 학습 및 분류에 적합)
- **라벨 인코딩:** `scikit-learn`의 `LabelEncoder` (문자열 라벨을 모델이 이해하는 숫자로 변환)
- **파이프라인 통합:** `scikit-learn`의 `Pipeline` (벡터화 및 분류 모델을 하나의 워크플로우로 묶어 관리)
- **데이터 저장:** `json`, `csv` (결과 데이터를 JSON 또는 CSV 형식으로 저장)

**3.2. 구현 전략:**

- **모듈화된 클래스:** `AutomatedDataPipeline`이라는 단일 클래스 내에 웹 수집, 정제, 모델 학습, 예측, 저장 등 모든 파이프라인 단계를 메서드로 캡슐화하여 코드의 재사용성과 관리 용이성을 확보합니다.
- **견고한 웹 스크래핑:** `User-Agent` 헤더 설정, 타임아웃, `try-except` 블록을 통한 예외 처리 등 웹 크롤링의 안정성을 최우선으로 고려합니다. `<p>` 태그 외에 `div` 클래스 등을 활용한 본문 탐색 시도를 포함하여 다양한 웹 구조에 대응합니다.
- **효율적인 텍스트 전처리:** 정규 표현식을 활용하여 HTML 태그, URL, 과도한 공백, 불필요한 특수문자 등을 효과적으로 제거하고, 의미 없는 짧은 문장은 필터링합니다.
- **통합된 머신러닝 워크플로우:** `scikit-learn`의 `Pipeline` 객체를 사용하여 텍스트 벡터화와 분류 모델 학습 및 예측 과정을 통합하여, 일관된 전처리-예측 흐름을 보장하고 코드 오류 가능성을 줄입니다.
- **모델 학습 및 예측 안정성:** 모델이 학습되지 않았을 경우, 예측을 시도할 때 명확한 오류 메시지(예: `ValueError`)를 발생시켜 개발자가 문제를 쉽게 인지할 수 있도록 합니다.
- **다양한 저장 형식:** JSON과 CSV 두 가지 형식으로 결과 데이터를 저장할 수 있도록 기능을 구현하여, 사용자의 데이터 활용 편의성을 높입니다.

---

### 4. 향후 확장 및 고도화 계획

MVP 개발 완료 후, 프로젝트의 요구사항이 더욱 복잡해지거나 성능 개선이 필요할 경우, 다음과 같은 점진적인 확장 및 고도화를 고려할 수 있습니다.

- **정교한 문장 분리:** 한국어 텍스트 처리를 위해 `NLTK`나 `KSS`와 같은 전문 자연어 처리 라이브러리의 문장 분리 기능을 도입하여 정확도를 높입니다.
- **고급 텍스트 전처리:** 명사 추출, 형태소 분석 등 한국어 특성을 고려한 전처리 기법을 도입하여 모델의 이해도를 높입니다.
- **모델 지속성:** 학습된 머신러닝 모델을 파일로 저장(`pickle` 또는 `joblib` 활용)하고 재사용하는 기능을 추가하여, 파이프라인 실행 시 매번 모델을 다시 학습하는 비효율을 제거합니다.
- **딥러닝 모델 연동:** 라벨링의 정확도를 극대화해야 할 경우, `sentence-transformers`를 활용한 임베딩 후 `LogisticRegression`을 사용하거나, `Hugging Face`의 경량화된 사전 학습 언어 모델(예: `DistilBERT`, `MiniLM`)을 파인튜닝하여 분류기로 활용하는 방안을 검토합니다.

---

이 문서는 프로젝트의 핵심적인 개발 방향성을 제시하며, 향후 개발 과정에서 발생할 수 있는 혼란을 최소화하고 일관된 개발을 유도할 것입니다.

</aside>


AI 학습용 자동 데이터 파이프라인 개발 사전 준비 사항
1. 개발 환경 설정 관련
    1.1. Python 버전
    Python 3.13.5 사용

    1.2. 통합 개발 환경 (IDE) 또는 텍스트 에디터
    VS Code (Visual Studio Code) 사용. 개인 프로젝트이므로 팀원과의 협업이나 코드 컨벤션 합의는 불필요.

    1.3. 가상 환경 (Virtual Environment) 관리
    venv (Python 기본 모듈)

2. 라이브러리 및 의존성 관리
    2.1. 주요 라이브러리 명시
    문서에 언급된 핵심 기술 스택은 다음과 같습니다:
    requests (정적 웹 데이터 수집)
    BeautifulSoup (웹 데이터 수집)
    ~~selenium (동적 웹 데이터 수집)~~
    re (텍스트 정제)
    scikit-learn (TfidfVectorizer, LogisticRegression, MultinomialNB, LabelEncoder, Pipeline 포함)
    json, csv (데이터 저장)

    2.2. 의존성 파일 (requirements.txt)
    필수: 개발을 시작하기 전에 이 라이브러리들의 특정 버전을 포함하는 requirements.txt 파일을 생성하고 관리해야 합니다. 이는 프로젝트의 환경을 명확히 하고, 나중에 다른 컴퓨터에서 프로젝트를 실행할 때 필요한 라이브러리들을 쉽게 설치할 수 있게 해줍니다.

3. 데이터 및 AI 모델 관련
    3.1. 초기 소규모 학습 데이터셋
    'AI 추론 모듈' 학습 방식 | 각 주제별 5~10개 문장를 즉석에서 학습
    준비 방식: 직접 수동으로 작성하며, 전용 양식(Format)을 따로 만들어 작성 후 학습 예정입니다.
    데이터 형식: 별도의 파일(예: JSON, CSV 또는 간단한 텍스트 파일)로 저장하여 관리합니다. (초기에는 파이썬 코드 내에 직접 정의할 수도 있으나, 추후 관리 및 확장을 위해 파일로 분리하는 것이 용이하다고 판단)

    3.2. 카테고리(라벨) 정의
    AI 모듈이 추론하여 부여할 '카테고리(라벨)'의 종류 정의 방법
    분류 기준 및 라벨명: 한국의 교과 과목을 최상위 부모로 하는 계층적 구조로 카테고리를 분류합니다. 예를 들어, 전문 교과를 시작으로 그 하위 계열(예: 공업계열), 그리고 더 구체적인 분야(예: 정보·컴퓨터), 최종적으로는 수집 대상 데이터의 가장 세부적인 항목(예: 그래픽카드)까지 정의합니다. 분류는 위키처럼 하위의 하위로 계속 내려가는 방식.
    전문 교과의 하위 예시 (수집 대상에 따라 선택):
    전기 (전기공학과 등), 전자 (전자공학과 등), 통신 (정보통신공학과 등), 기계 (기계공학과 등), 재료 (신소재공학과 등), 화공 (화학공학과 등), 섬유 (섬유공학과 등), 자원, 건설 (건축공학과, 도시학과, 토목공학과 등), 냉동, 세라믹, 인쇄 (출판인쇄과), 정보·컴퓨터 (컴퓨터공학과), 디자인 (디자인학과 등)
    예시 분류 경로:그래픽카드
    전문 교과 -> 공업계열 -> 정보·컴퓨터 -> PC자원 -> PC 하드웨어 -> GPU -> 그래픽카드
    이러한 계층 구조를 통해 데이터에 라벨을 부여하며, AI는 이 계층적 분류 체계를 학습하여 데이터를 분류.

    3.3. 웹 데이터 수집 대상
    정보 수집 사이트 관련

    구체적인 URL:
    1순위: 나무위키, Wikipedia 등 위키류 사이트를 우선 참조합니다.
    2순위: PC 관련 프로젝트의 경우, 퀘이사존, 쿨엔조이 같은 국내 IT/하드웨어 커뮤니티 사이트를 활용합니다. PC 이외 주제의 경우, 해당 주제에 맞는 전문 커뮤니티 사이트 또는 4chan, Reddit 같은 해외 대형 커뮤니티 사이트도 고려합니다.
    3순위: IT동아, 케이벤치 같은 IT 전문 뉴스/리뷰 웹진을 활용합니다.

    수집 전략:
    AI 모델이 내용을 파악하여 분류할 수 있도록, 순수 텍스트 본문을 최대한 정확하게 추출하는 것이 가장 중요합니다. 개인 프로젝트의 효율성과 구현 용이성에 초점을 맞춥니다.
    핵심 원칙: 정확한 본문 추출, 기본적인 안정성 확보, 법적 준수
    URL 수집 및 관리: 대상 웹사이트의 주요 카테고리 페이지나, 직접 파악한 관심 페이지의 URL들을 초기 수집 대상으로 확보합니다. 수집할 URL들은 간단한 파이썬 리스트나 텍스트 파일 등으로 관리하고, 수집 완료 여부는 파일 저장 등으로 확인합니다.

    정교한 개별 페이지 접근 및 본문 추출: BeautifulSoup을 사용하여 HTML을 파싱하며, 수집할 특정 사이트의 HTML 구조를 직접 분석하여 본문 추출 규칙을 정의합니다. (필요시 각 사이트별로 다른 추출 로직 구현) 게시글, 기사, 위키 페이지의 본문 텍스트(주로 <p> 태그나 특정 div 클래스/ID를 가진 부분)를 가져오는 데 집중하며, 댓글, 광고, 내비게이션 바, 푸터, 사이드바 등 분류에 방해가 되는 불필요한 요소는 제거합니다. 본문 텍스트 외에 제목, 작성일, 원본 URL 등 필요하다고 판단되는 메타 정보도 함께 추출하여 데이터셋에 포함할 수 있습니다. 추출된 텍스트에서 HTML 태그 잔여물, 불필요한 공백, 개행 문자, 특수 문자 등을 정규 표현식(re 모듈)을 활용하여 깨끗하게 정제합니다.

    기본적인 크롤링 운영 및 윤리적/법적 준수:
    robots.txt 준수: 대상 웹사이트의 robots.txt 파일을 확인하여, 크롤링이 허용된 범위 내에서만 데이터를 수집합니다.
    User-Agent 설정: requests 요청 시 일반 웹 브라우저와 유사한 User-Agent 문자열을 사용하여 기본적인 웹 크롤러임을 명시합니다.
    간단한 요청 지연: 웹 서버에 무리를 주지 않도록 요청 사이에 짧은 시간 지연(time.sleep(0.5) 등)을 두는 것이 좋습니다.
    간단한 예외 처리: 네트워크 오류, HTTP 오류(예: 404 페이지 없음), 타임아웃 등 기본적인 오류 상황에 대비하여 try-except 구문을 활용합니다. 간단한 로깅(logging 모듈)을 통해 어떤 URL에서 오류가 발생했는지 기록할 수 있습니다.
    데이터 사용 목적 명시: 수집한 데이터를 어떤 목적으로 사용할 것인지 명확히 하고, 상업적 이용 등 제한이 있는 데이터는 수집하지 않는 등 저작권 및 이용 약관을 준수합니다.
    크롤링 정책 확인: 해당 웹사이트들의 robots.txt 파일을 반드시 확인하여, 허용되는 범위 내에서만 데이터를 수집할 것입니다. 과도한 요청으로 인한 차단을 피하기 위해 요청 간 적절한 딜레이를 둘 예정입니다.

    3.4. 중간 데이터 저장(2차 정제 이전)
    목적: 웹에서 수집된 원시 데이터가 1차 정제(HTML 태그 제거, 공백 정리 등)를 거친 후, AI 라벨링 및 2차 정제와 같은 다음 단계로 넘어가기 전의 중간 결과를 안정적으로 보관하기 위함입니다. 이는 파이프라인의 특정 단계에서 문제가 발생했을 때, 이전 단계부터 다시 시작하는 대신 중간 저장된 데이터부터 작업을 재개할 수 있게 해주는 '체크포인트' 역할을 합니다.
    저장 시점: 데이터 수집 및 1차 정제 직후에 이루어집니다.
    데이터 형식: 파일(예: JSON Lines 형태의 .jsonl 파일) 형태로 저장하여 관리할 예정입니다. (크롤링 데이터는 양이 많아질 수 있으므로 파이썬 코드 내 정의보다는 파일 저장이 효율적입니다.)
    저장 경로: 예를 들어, data/intermediate/crawled_raw_text_data.jsonl 와 같이 원시 정제 데이터용 별도 중간 저장 경로를 사용할 수 있습니다.

    3.5. 최종 데이터 저장 형식 및 경로
    저장 형식: JSON을 기본으로 사용하고, 필요시 CSV로 변환할 수 있도록 고려할 것입니다. (JSON Lines (.jsonl) 형식이 효과적일 수 있습니다.)
    저장 경로: 카테고리별로 분리해서 저장할 예정입니다. (예: data/output/전문교과/공업계열/정보_컴퓨터/PC자원/PC_하드웨어/GPU/그래픽카드_data.jsonl 등)

4. 코드 관리 및 협업
    4.1. 버전 관리 시스템 (VCS)
    필수: Git을 사용하여 코드 버전을 관리하고 GitHub, GitLab, Bitbucket 등 원격 저장소를 활용해야 합니다.
    결정 사항: 개인 프로젝트이므로 협업 목적은 아니지만, 코드 이력 관리 및 포트폴리오를 위해 GitHub 사용을 강력히 권장합니다.

    4.2. 코드 컨벤션
    PEP 8과 같은 Python 코딩 스타일 가이드를 따르거나, 팀 내에서 자체적인 코드 컨벤션을 정의하여 일관된 코드 품질을 유지해야 합니다.
    결정 사항: 개인 프로젝트이므로 엄격할 필요는 없으나, 가독성 높은 코드를 위해 PEP 8 가이드를 참조하여 작성하는 것이 좋습니다.

    4.3. 문서화
    코드에 주석을 충실히 달고, README 파일 등을 통해 프로젝트의 설치 및 사용법, 주요 기능 등을 문서화하는 계획을 세워야 합니다.
    결정 사항: README.md 파일 작성을 통해 프로젝트의 개요, 설치/실행 방법, 주요 기능 등을 기록하는 것을 권장합니다. 이는 나중에 프로젝트를 다시 보거나 다른 사람에게 설명할 때 큰 도움이 됩니다.

5. 테스트 및 검증 계획
    5.1. 단계별 테스트
    데이터 수집, 정제, 라벨링, 저장 각 단계별로 올바르게 작동하는지 확인할 테스트 계획.
    각 단계의 중간 결과를 print 문으로 출력하여 눈으로 확인하고, 필요한 경우 중간 파일을 생성하여 내용을 직접 확인,
    과정이 완료되면 logging 모듈을 사용하여 기록된 로그를 통해 전체 진행 상황과 오류 여부를 확인.

    5.2. 라벨링 정확성 검증
    'AI 추론 모듈'의 핵심 목표가 "데이터 라벨링의 정확성"이므로, 라벨링 결과의 정확도 측정 및 검증 방안

    기준 및 방법: 직접 소수의 '정답 데이터(골드 스탠다드)'를 만들어서 모듈의 예측 결과와 비교. scikit-learn의 정확도 계산 함수(accuracy_score, classification_report 등)는 참고용으로 활용하고, 결과가 잘됬다고 판단되면 진행.

