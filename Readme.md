AI 학습용 자동 데이터 파이프라인 (Automated AI Data Pipeline)
🌟 프로젝트 개요
이 프로젝트는 AI 학습에 필요한 텍스트 데이터를 자동으로 수집하고, 초기 정제를 거쳐, AI 모델이 활용하기 쉬운 형태로 가공하는 자동화된 파이프라인입니다. 웹 페이지에서 콘텐츠를 가져와 필요한 정보를 추출하고 구조화하는 것을 목표로 합니다.

궁극적인 목표는 수집된 데이터를 AI 모델이 스스로 내용을 '추론'하여 카테고리(라벨)를 부여하고, AI 학습에 바로 사용될 수 있는 정돈된 데이터셋으로 최종 가공하는 것입니다. 현재 버전에서는 데이터 수집 및 가공(키워드) 단계까지 구현.

✨ 주요 특징
웹 데이터 수집 (Web Data Collection): requests, BeautifulSoup라이브러리를 활용하여 지정된 URL의 웹 페이지 HTML 콘텐츠를 가져옵니다.

Robots.txt 준수: urllib.robotparser 표준 라이브러리를 사용하여 웹사이트의 robots.txt 규칙을 확인하고 준수하여 법적 크롤링을 준수합니다.

초기 텍스트 정제: HTML 태그, 불필요한 공백, URL, 특수 문자 등을 제거하여 깨끗한 본문 텍스트를 추출합니다.

텍스트 특징 추출 및 요약 (TF-IDF Keywords & Snippet Generation):

TF-IDF (Term Frequency-Inverse Document Frequency) 기법을 사용하여 각 문서의 핵심 키워드를 추출합니다.

중간 데이터 저장 (Intermediate Data Storage): 수집 및 가공된 데이터를 JSON Lines (.jsonl) 형식으로 저장하며, 필요한 폴더를 자동으로 생성합니다.

⚙️ 파이프라인 단계
우리 프로젝트의 **'자동화 파이프라인'**은 다음과 같은 단계들로 구성됩니다.

데이터 수집
↓
초기 데이터 정제
↓
텍스트 특징 추출 및 요약 (저수준 키워드) 🌟 (현재 구현 완료)
↓
(이후 구현 목표) 콘텐츠 기반 자동 라벨링 (AI 추론)
↓
최종 데이터셋 가공 및 저장 (카테고리별)

데이터 수집 (Web Crawling): 웹에서 원본 HTML 콘텐츠를 가져오는 단계입니다. robots.txt를 준수하며 웹 서버에 부담을 주지 않도록 지연 시간을 설정합니다.

초기 데이터 정제: 수집된 HTML에서 본문 텍스트를 추출하고, HTML 태그, 불필요한 공백, URL 등 AI 학습에 방해가 되는 요소를 제거하여 깨끗한 텍스트를 만듭니다.

텍스트 특징 추출 및 요약 (TF-IDF 키워드): 정제된 텍스트에서 TF-IDF 기법으로 핵심 키워드 5개를 추출해 데이터에 추가합니다.

(구현 목표) 콘텐츠 기반 자동 라벨링: 이 단계는 파이프라인 내에서 AI의 '추론'이 핵심적으로 작동할 부분입니다. 중/소 규모의 수동 분류된 학습 데이터를 바탕으로 머신러닝 분류 모델을 학습하고, 이를 통해 새로 수집된 문서에 가장 적합한 주제 카테고리(라벨)를 자동으로 부여합니다.

 최종 데이터셋 가공 및 저장: 라벨링된 데이터를 AI 모델이 쉽게 불러와 학습할 수 있는 표준화된 형식으로 최종 변환하여, 정의된 카테고리별 계층적 경로에 저장합니다.

🚀 설치 및 실행 방법
이 프로젝트를 로컬 환경에서 실행하기 위한 단계별 가이드입니다.

1. 사전 준비 사항
Python 3.x: 시스템에 Python이 설치되어 있어야 합니다. (Python 3.8 이상 권장)

2. 저장소 클론 (Clone) 또는 다운로드
명령 프롬프트(CMD) 또는 터미널을 열고 다음 명령어를 실행하여 프로젝트를 로컬로 가져옵니다:
git clone https://github.com/DELTA0927/auto-ai-data-pipeline.git
필요한 라이브러리 설치
프로젝트에 필요한 모든 Python 라이브러리(종속성)는 requirements.txt 파일에 정의되어 있습니다. 

가상 환경을 활성화한 상태에서 다음 명령어를 실행하여 설치합니다:
가상 환경 생성:
        python -m venv venv
가상 환경 활성화:
        * **Windows:**
            .\venv\Scripts\activate
        * **macOS/Linux:**
            source venv/bin/activate
        (가상 환경 활성화 시 터미널 프롬프트 앞에 `(venv)`가 표시됩니다.)

pip install -r requirements.txt
pipeline.py 스크립트를 실행하여 데이터 수집 및 가공 파이프라인을 시작할 수 있습니다.
참고: pipeline.py 파일 내 test_urls 리스트를 수정하여 수집하고자 하는 웹 페이지의 URL을 변경할 수 있습니다.

🛠️ 기술 스택
Python 3.x: 주력 프로그래밍 언어
Requests: 웹 페이지의 HTML 콘텐츠를 가져오는 HTTP 요청 라이브러리
BeautifulSoup4: HTML 및 XML 파일을 파싱하여 필요한 데이터를 추출하는 라이브러리
Scikit-learn: 텍스트 특징 추출(TfidfVectorizer) 및 향후 머신러닝 분류 모델 구현을 위한 라이브러리
Urllib.robotparser: 웹사이트의 robots.txt 파일을 파싱하여 크롤링 규칙을 준수
JSON: 데이터 저장 형식

📊 데이터 저장 형식
수집된 데이터는 data/intermediate 폴더 내에 JSON Lines (.jsonl) 형식으로 저장됩니다. 파일명은 crawled_processed_data_YYYYMMDD_HHMMSS.jsonl 형태로 생성됩니다.

📈 향후 개선 및 확장 계획
이 파이프라인은 지속적으로 발전할 예정이며, 다음 단계에서는 주로 AI 추론(자동 라벨링) 기능의 통합과 데이터 전처리/저장 방식의 고도화에 초점을 맞출 것입니다.
AI 분류 모델 통합: scikit-learn의 LogisticRegression 또는 MultinomialNB와 같은 분류 모델을 학습시키고, 새로운 문서에 자동으로 주제 카테고리(라벨)를 부여하는 기능을 추가합니다.
고급 텍스트 전처리: 한국어 특성을 고려한 명사 추출, 형태소 분석 기법을 도입하여 모델의 텍스트 이해도를 높입니다.
본문 추출 로직: 특정 웹사이트의 HTML 구조를 직접 분석하여 본문 추출 규칙을 정의하거나, 댓글/광고 등 불필요한 요소를 더욱 효과적으로 제거하는 로직을 추가합니다.
메타 정보 추출: 본문 텍스트 외에 제목, 작성일 등 웹 페이지의 추가적인 메타 정보를 함께 추출하여 데이터셋에 포함합니다.
카테고리별 계층적 저장 경로: AI 분류 모델로 라벨링된 데이터를 최종적으로 각 카테고리에 맞는 계층적인 폴더 구조(data/output/카테고리1/서브카테고리/...)로 자동 저장합니다.
모델 지속성: 학습된 머신러닝 모델을 저장하고 재사용하는 기능을 추가하여, 파이프라인 실행 시 매번 모델을 다시 학습하는 비효율을 제거합니다.